<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html">




<head>
<!-- Meta Tag -->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    
    <!-- SEO -->
    <meta name="description" content="codingPrivacy is an opensource community to share knowledge on many different topics such as Machine Learning, python, GUI etc.">
    <meta name="author" content="codingprivacy">
    <meta name="url" content="codingprivacy.github.io">
    <meta name="copyright" content="codingprivacy">
    <meta name="robots" content="index,follow">
    
    
    <title>Backpropagation - A pillar of neural networks</title>
    
    <!-- Favicon -->
    <link rel="shortcut icon" href="http://localhost:4000/img/favicon.png">
    <link rel="logo" sizes="144x144" type="image/x-icon" href="http://localhost:4000/img/favicon.png">
    
    <!-- All CSS Plugins -->
    <link rel="stylesheet" type="text/css" href="http://localhost:4000/css/blog_plugin.css">
    
    <!-- Main CSS Stylesheet -->
    <link rel="stylesheet" type="text/css" href="http://localhost:4000/css/blog_style.css">
    
    <!-- Google Web Fonts  -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Poppins:400,300,500,600,700">
    
</head>
    


    
    
 <body>

<!-- Preloader Start -->
     <div class="preloader">
	   <div class="rounder"></div>
      </div>
     
      
      
    
    
    <div id="main">
        <div class="container">
            <div class="row">
            	 <div class="col-md-9">
                  <div class="col-md-12 page-body">
                      <div class="row">
                    		
                            
                            <div class="sub-title">
                                
                           		<h1><b>Backpropagation - A pillar of neural networks</b></h1>
                                
                               <span><p >April 21, 2020</p></span>
                               <p >Harsh Patel</p>
                             </div>

                          <div class="col-md-12 content-page">
                            	
                                
                                
                                <div class="col-md-12 blog-post">
                                    

                                    
                                	
     
	 

 <p>Hello people, this blog is about the backpropagation algorithm used in Machine learning. It is one of the most important algorithms and can be described as the pillar of the neural networks. I tried to make it as simple as possible, so I am sure you would be able to understand the fundamental working of backpropagation after reading this post.</p>
<br>
<h2>Overview</h2>
<p>As we have seen in the last post that during training, Neural networks undergo feedforward and backpropagation mechanism. Data moves from input to output i.e. first layer to the last layer in feedforward mechanism and then we start backward and move from right to left to again first layer in backpropagation. The main purpose of backpropagation is that we update other parameters of the network with a change in a loss. </p>
<br>
<h2>Introduction</h2>
<p>From input data, we generate predicted data using feedforward propagation. then we find a loss between predicted data and true data. Now as loss gets updated, we need to update all network parameters before the next iteration. So, using backpropagation, we update all the values for the change in a loss. We use chain rule in derivation to calculate this.
</p>
<br>
<h2>Feedforward at a glance</h2>
<p>Notations: </p>
    <div style="text-align-last: start;">
        <h4>
        $$ T = \text{true data} $$
        $$ L = loss $$
        $$ Y = \text{predicted data} $$
        $$ W = \text{weights ( }W_1 \text{ means layer 1 weights) } $$
        $$X = \text{input data} $$
        $$B = bias $$  
        </h4>
    </div>

    
    <h4>$$ Z_1 = X \cdot W_1 + B_1 $$</h4>
    <h4>$$ H_1 = \sigma(Z_1) $$</h4>
    <h4>$$ Z_2 =  H_1 \cdot W_2 + B_2 $$</h4>
    <h4>$$ Y = \sigma(Z_2) $$</h4>
    <h4>$$ L = \frac{1}{N} \sum_{i=0}^{N} (Y-T)^{2}  $$</h4>
    <br>

    <center>
         <img src="http://localhost:4000/img/backpropagation/feedforward.PNG" class="img-responsive" height="70%" width="70%">
        <caption>figure 1. Computation graph for single layer network</caption>
    </center>
    <br>

<p>The above computation graph shows single hidden layer working of the network. If you are not able to understand it, you can know more about the feedforward network on my previous blog.  <b  style="color: #1abc9c;"><a href="../../blogs/feedforward-propagation">click here</a></b>.</p>
<br>        
<h2>Backpropagation</h2>
<p>
    Backpropagation is calculated for \(n+1\) times for \(n\) layer network. In other words,  <b  style="color: #1abc9c;"> No. of backpropagation = No. of weights in the network </b> so there will be 2 times backpropagation according to above network.
Now Let’s look at backpropagation for \(W_2\) (last weight) in the network. As we have discussed before, we have to find the change of other values with change in loss.<br>
</p>
    <center>
         <img src="http://localhost:4000/img/backpropagation/first_layer.PNG" class="img-responsive" height="70%" width="70%">
        <caption>figure 2. Computation graph showing first backpropagation</caption>
    </center>
<h4>
<p>Last layer backpropagation includes all the highlighted circles as shown in the above figure 2. So the values we need to find are:</p>
</h4>
<div style="text-align-last: start">
    <h4>
        $$    \frac{\partial L}{\partial Y} = \text {change of Y with loss}  $$
        $$    \frac{\partial Y}{\partial Z_2} = \text {change of Z with Y}     $$
        $$    \frac{\partial Z_2}{\partial W_2} = \text {change of W with Z}     $$
        $$    \frac{\partial Z_2}{\partial B_2} = \text {change of B with Z}     $$
    </h4>
</div>
<h4>
$$ 
\frac{\partial L}{\partial Y} , 
 \frac{\partial Y}{\partial Z_2} , 
 \frac{\partial Z_2}{\partial W_2} ,
 \frac{\partial Z_2}{\partial B_2}
$$
</h4>
<p>Of course, we can find all these values, but here is a question, what values can we update after calculating them? We cannot update \(Y\) or \(Z\) as it is generated by the model. We can change bias and weights as we initialize it from the start. Bias and weights does gets manipulated  throughout the layers. Weights decide how many features should be propagated further. We can also not change input data as changing it will not be a good idea ever (we want to find a solution to the problem, not change the problem). So we are going to update only these values.
</p>

<h4>
$$ \frac{\partial Z_2}{\partial W_2} , \frac{\partial Z_2}{\partial B_2} $$ 
</h4>

<p>
To compute this, we will apply the chain rule.
Chain rule works as follows:
</p>

<h4>
$$ \frac{\partial a}{\partial d} = \frac{\partial a}{\partial b} \times \frac{\partial b}{\partial c} \times \frac{\partial c}{\partial d} $$ </h4>

<p>Hence applying it in our scenario: </p>
<h4>
$$    \frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial Y} \times \frac{\partial Y}{\partial Z_2} \times \frac{\partial Z_2}{\partial W_2} \quad,\quad \frac{\partial L}{\partial B_2} = \frac{\partial L}{\partial Y} \times \frac{\partial Y}{\partial Z_2} \times \frac{\partial Z_2}{\partial B_2}$$
</h4>

<h3>An example</h3>
<p>
This is like passing a message to each node backward. Let us take a small example to understand this better. Suppose you are going on a trip to Las Vegas with a budget of 30K. Now you want to know how much money you will be left with for using in casino eliminating other expenses. You have an idea that 10% will be travel cost and 20% will be accommodation cost. 
Let us compute this problem and help you out with it. <br>
Budget = 30K <br>
travel cost = 30K * 0.10 = 3K <br>
accommodation cost = 30K * 0.20 = 6K <br>
Money after eliminating costs = 21K <br>
Hence you will have 21K to use In the casino. Now if you change the budget, you would have to compute all the costs again to calculate remaining money. <br><br>
Same way as you find loss, you need to change all the intermediate values which finally gives you the changes in weights and bias. This would have given you an idea of why and what happens.
</p>

<h3>Continuing further</h3>
<p> Now let us calculate chain rule derivatives.<br>
</p>

<h4>
\( \text{Sigmoid as an activation function } \sigma = \frac{e^x}{(1 + e^{-x})} \)<br><br>
\( \text{derivative of sigmoid } \sigma = \sigma' = \sigma(x) \times (1 - \sigma(x)) \)<br><br>
\( \text{MSE loss } = \frac{1}{N} \sum_{i=0}^{N} (y-t)^{2} \) <br><br><br>
</h4>

<center>
         <img src="http://localhost:4000/img/backpropagation/table1.PNG" class="img-responsive" height="50%" width="50%">
         <caption>table 1. Calculation of derivatives for first backpropagation</caption>
</center>
<br>
<p>Using table 1. we get the following values. </p>
<h4>
$$ \frac{\partial L}{\partial W_2} = (Y - T) \times \sigma'(Z_2) \times X $$
$$ \text{As } \frac{\partial Z_2}{\partial B_2} = 1, \quad \frac{\partial L}{\partial B_2} = (Y - T) \times \sigma'(Z_2) \quad \text{i.e. equal to }\ \frac{\partial L}{\partial Z_2} $$
</h4>
    
<p>We completed \(1\) backpropagation , we will now move further and calculate for \(W_1\) (i.e. the first weight of our network) and then generate a formula for the arbitrary number of layers.</p>
<p>
    Till now, we have computed</p> <br>
<h4>
    $$ \frac{\partial L}{\partial Z_2} = (Y - T) \times \sigma'(Z_2) $$
    $$ \frac{\partial L}{\partial W_2} = (Y - T) \times \sigma'(Z_2) \times X $$
</h4>

<center>
         <img src="http://localhost:4000/img/backpropagation/second_layer.PNG" class="img-responsive" height="80%" width="80%">
         <caption>figure 3. Computation graph showing second backpropagation</caption>
</center>
<br>
<p>Applying Chainrule for \(2^{nd} \) backpropagation :-</p>

$$ \frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial Y} \times \frac{\partial Y}{\partial Z_2} \times \frac{\partial Z_2}{\partial H_1} \times \frac{\partial H_1}{\partial Z_1} \times \frac{\partial Z_1}{\partial W_1} $$

$$ \frac{\partial L}{\partial B_1} = \frac{\partial L}{\partial Y} \times \frac{\partial Y}{\partial Z_2} \times \frac{\partial Z_2}{\partial H_1} \times \frac{\partial H_1}{\partial Z_1} \times \frac{\partial Z_1}{\partial B_1} $$

<p>Because we already know \( \frac{\partial L}{\partial Z_2} \)from previous calculations, we can simplify this as,</p>

$$ \frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial Z2} \times \frac{\partial Z_2}{\partial H_1} \times \frac{\partial H_1}{\partial Z_1} \times \frac{\partial Z_1}{\partial W_1} $$

$$ \frac{\partial L}{\partial B_1} = \frac{\partial L}{\partial Z2} \times \frac{\partial Z_2}{\partial H_1} \times \frac{\partial H_1}{\partial Z_1} \times \frac{\partial Z_1}{\partial B_1} $$
<br>
<center>
         <img src="http://localhost:4000/img/backpropagation/table2.PNG" class="img-responsive" height="80%" width="80%">
         <caption>table 2. Calculation of derivatives for second backpropagation</caption>
</center>

<br>
<p>Using table 2. we get the following values. </p>
<h4>
$$ \frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial Z_2} \times W_2 \times \sigma'(Z_1) \times X  $$
$$ \frac{\partial L}{\partial B_1} = \frac{\partial L}{\partial Z_2} \times W_2 \times \sigma'(Z_1) \times 1 $$
</h4>
<p>expanding we get,</p>

<h4>

$$ \frac{\partial L}{\partial W_1} = (Y - T) \times \sigma'(Z_2) \times W_2 \times \sigma'(Z_1) \times X  $$
$$ \frac{\partial L}{\partial B_1} = (Y - T) \times \sigma'(Z_2) \times W_2 \times \sigma'(Z_1) \times 1 $$
</h4>

<p>So, we calculated all required derivatives required for backpropagation for a single layer neural network.</p>
<br>
<h2>Arbitrary number of layers</h2>

<p>We can observe that each time we calculate backpropagation, we already have some values of the previous calculations in the chainrule. So we can and keep multiplying further with those values as we go on. we notice that in both \( \frac{\partial L}{\partial W_1}\) and \(\frac{\partial L}{\partial W_2}\), we have some common values. The common values are :</p>
<h4>
    $$ (Y - T) \times \sigma'(Z_2) \times X $$
</h4>
<p> and what gets added in second backpropagation is :</p>
<h4>
    $$ W_2 \times \sigma'(Z_1) $$
</h4>
<p>When we also calculate for 3rd backpropagation in a more dense network, we find out that each time this same values keeps adding up for each layer and the common value remains fixed. Using this observations we derive a general formula for any network as the following.</p>
<div style="text-align-last: start">
    <h5>
        $$ n = \text{total hidden layers of network} + 1 = \text{total weights of the network} $$
    </h5>
</div>
<h4>
$$ \frac{\partial L}{\partial B_k} = (Y - T) \times \sigma'(Z_n) \times \prod_{i=2}^{k} {W_{k} \times \sigma'(Z_{k-1})} $$
$$ \frac{\partial L}{\partial W_k} = (Y - T) \times \sigma'(Z_n) \times X \times \prod_{i=2}^{k} {W_{k} \times \sigma'(Z_{k-1})} $$
</h4>
<br>
<h2>Some depth</h2>
<p>
This \( \frac{\partial L}{\partial W_n} \)  and \( \frac{\partial L}{\partial B_n} \) are the gradients of the networks.
Mathematically these gradients are vectors that move in a specific direction to reduce the loss of the network. If we suppose our training data have 2 features \(X_1\) and \(X2\), the contour graph can be used for this visualization. The contour graph is a \(2d\) representation of \(3d\) surface. \(X_1\) and \(X_2\) are the features in the \(X\) and \(Y\) axis respectively and bumps (\(3^{rd}\) surface) represents a loss of the network given \(X_1\), \(X_2\) values. The peak represents high loss and base represents low. These gradients try to move to global minima which is the lowest point of the surface. Thus we aim to go downward.
<br><br>
Our gradient is a vector which is a slope of the surface. As slope \( m = \frac{y}{x} \), we do the same calculation in finding the derivative. Hence It is simply a slope. So we keep finding the slope of the surface, update our values and move along the downward direction. 
</p>
<br>
<h2>What after finding these gradients</h2>
<p>
As these weights and bias are initialized randomly before feedforward propagation, we update them after backpropagation. </p>
<h4>
    $$ W_{\Delta} = \frac{\partial L}{\partial W_n} $$
    $$ B_{\Delta} = \frac{\partial L}{\partial B_n} $$
    $$ W_{new} = W_{current} - W_{\Delta} $$
    $$ B_{new} = B_{current} - B_{\Delta} $$
</h4>
<p>
The reason we subtract the \(W_{\Delta}\) from the current value is that we need to go to the opposite of the slope to reach global minima.
Let's take an example of \(m = 2x\). \( \frac{\partial m}{\partial x} = 2 \) which is the gradient. Now increasing \(x\) by \(2\) will always increase the value of \(m\). Therefore we subtract it from the current value.
</p>
<br>
<h2>steps of the neural network</h2>
<p>
iterate \(n\) times:  <br>
</p>
<pre>1.	Feedforward propagation</pre>
<pre>2.	Find loss between predicted images and true images</pre>
<pre>3.	Backpropagation </pre>
<pre>4.	Update the weights based on the gradients</pre>

<br>
<h2>Conclusion</h2>
<p>This is all about backpropagation. In the upcoming posts, I will show you how to program a simple neural network from scratch and then using higher-level libraries.</p>
                                

               

                                    

                                </div>
                            </div>
                             <br>                     
                                
                                
                               
                       
                       <div class="col-md-12 page-body margin-top-50 footer">

  
  
                          <footer>

                          <ul class="menu-link">
                               <li><a href="https://codingprivacy.github.io">Home</a></li>

                               <li><a href="http://localhost:4000/about">Contact</a></li>
                            </ul>
                            
                          <p>© Copyright 2018 Coding Privacy. All rights reserved</p>
						  
						  
						  

                           
                         </footer>
                       </div>
                       <!-- Footer End -->
                     
                     
                  
                
            </div>
         </div>
      </div>

      <div class="col-md-3">
                        <div>
                        <a href="https://codingprivacy.github.io">
                          <div class="right_bar">
                          <h3><b>CodingPrivacy</b></h3>
                          </div> 
                        </a>
                        </div>
                      
                      
                      
                      <div class="my-detail">

                       
                       <ul class="social-icon">
                         <li>
                             
                             <a href="https://github.com/codingprivacy/"><img src="http://localhost:4000/img/icons/github-sign.png">
                             </a>
                         </li>
                         <li><a href="https://www.youtube.com/channel/UChav2OCtkA8tAVY3D7OJlcA" ><img src="http://localhost:4000/img/icons/youtube-logo.png">
                             </a>
                       </li>
                        </ul>
                       <div id="links">
                           <p style="font-size: 15px;">Find more blogs:</p>
                        </div>
                          
                        
                        <div class="series-links" id="series-link" style="position: relative;">
                            
<!--                                -->
                            <a href="../backpropagation" style="font-size: 14px; margin: 7px;">Backpropagation - A pillar of neural networks</a><br><br>
<!--                                -->
                            
<!--                                -->
                            <a href="../feedforward-propagation" style="font-size: 14px; margin: 7px;">Feedforward propagation - A Simple Introduction to neural networks</a><br><br>
<!--                                -->
                            
<!--                                -->
                            <a href="../tkinter6" style="font-size: 14px; margin: 7px;">GUI Developement Using Python - 6. Button widget</a><br><br>
<!--                                -->
                            
<!--                                -->
                            <a href="../tkinter5" style="font-size: 14px; margin: 7px;">GUI Developement Using Python - 5. Place method (Tkinter geometry Manager)</a><br><br>
<!--                                -->
                            
<!--                                -->
                            <a href="../tkinter4" style="font-size: 14px; margin: 7px;">GUI Developement Using Python - 4. Grid method (Tkinter geometry manager)</a><br><br>
<!--                                -->
                            
<!--                                -->
                            <a href="../tkinter3" style="font-size: 14px; margin: 7px;">GUI Developement Using Python - 3. More about Pack method (Tkinter Geometry manager)</a><br><br>
<!--                                -->
                            
<!--                                -->
                            <a href="../tkinter2" style="font-size: 14px; margin: 7px;">GUI Developement Using Python - 2. Pack method (Tkinter Geometry manager)</a><br><br>
<!--                                -->
                            
<!--                                -->
                            <a href="../tkinter1" style="font-size: 14px; margin: 7px;">GUI Developement Using Python - 1. Starting Tkinter</a><br><br>
<!--                                -->
                            
<!--                                -->
                            <a href="../multiple_rename" style="font-size: 14px; margin: 7px;">Multiple file/folder rename using Python.</a><br><br>
<!--                                -->
                            
                            
                     </div>

                    </div>

                     
                  </div>
                  
                </div>
                  

<!-- Back to Top Start -->
            <a href="#" class="scroll-to-top"><i class="fa fa-long-arrow-up"></i></a>
            <!-- Back to Top End -->


            <!-- All Javascript Plugins  -->
            <script type="text/javascript" src="http://localhost:4000/js/blog_js/jquery.min.js"></script>
            <script type="text/javascript" src="http://localhost:4000/js/blog_js/plugin.js"></script>

            <!-- Main Javascript File  -->
            <script type="text/javascript" src="http://localhost:4000/js/blog_js/scripts.js"></script>
    
            <!-- Math equations library  -->
            <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    
    
    

                          
                          
            

   </body>
 </html>
